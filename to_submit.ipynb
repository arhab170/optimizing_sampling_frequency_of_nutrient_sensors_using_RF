{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c828980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "import plotly.express as px\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import jupyternotify\n",
    "ip = get_ipython()\n",
    "ip.register_magics(jupyternotify.JupyterNotifyMagics)\n",
    "from  sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import SVR\n",
    "import dateutil\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from mlxtend.evaluate import PredefinedHoldoutSplit\n",
    "import kneed\n",
    "import kaleido\n",
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee5b8669",
   "metadata": {},
   "source": [
    "station = \"erlabrunn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62250fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "station = \"kahl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONS:  \"OPO4P\", \"NO3N\", \"NH4N\", \"TRP\"\n",
    "predict = \"NO3N\"\n",
    "test_model = \"rf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff893a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model == \"rf\":\n",
    "    algo = RandomForestRegressor(n_jobs = -1)\n",
    "elif test_model == \"lr\":\n",
    "    algo = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ec63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are inputting the cleaned data that has been saved\n",
    "df = pd.read_csv('%s_%s_final.csv' %(station, predict))\n",
    "df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "df= df.set_index(\"Datum\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497377ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell splits the data into 80:20 ratio, for training and testing. Training dataset is named manual_train\n",
    "# testing dataset is named manual_test\n",
    "\n",
    "list_feat = df.columns.drop(predict)\n",
    "ss = df.reset_index()\n",
    "    \n",
    "column_headers = [x for x in ss.columns]\n",
    "time_df = pd.DataFrame(columns= column_headers)\n",
    "\n",
    "last_date = ss.iloc[-2,0]\n",
    "temp_date = ss.iloc[0,0]\n",
    "i = 0\n",
    "while temp_date.month != last_date.month or temp_date.year != last_date.year:\n",
    "    temp_date = ss.iloc[0,0] + pd.DateOffset(months =i)\n",
    "    temp = ss[(ss.Datum.dt.month == temp_date.month) & (ss.Datum.dt.year==temp_date.year) ]\n",
    "                \n",
    "    temp2 = temp.sample(frac = 20*0.01, random_state = 1)\n",
    "            \n",
    "    time_df = pd.concat([time_df, temp2], axis = 0)\n",
    "    i = i+1\n",
    "\n",
    "\n",
    "time_df = time_df.drop_duplicates(subset=['Datum'], keep='first')\n",
    "dates = np.array(time_df[\"Datum\"])\n",
    "time_df= time_df.set_index(\"Datum\")\n",
    "manual_train = ss.set_index(\"Datum\")\n",
    "manual_train.drop(dates, inplace=True)\n",
    "\n",
    "manual_test = time_df\n",
    "\n",
    "# here both the test and train dataframe are included with timestamps"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f66306db",
   "metadata": {},
   "source": [
    "#This is to save the un-seen data so that everytime it's not random.\n",
    "manual_test.to_csv(r'%s_%s_test.csv' %(station, predict), index = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to load a predefined manual_test value, so that it's not random everytime\n",
    "manual_test = pd.read_csv('%s_%s_test.csv' %(station, predict))\n",
    "manual_test['Datum'] = pd.to_datetime(manual_test['Datum'])\n",
    "manual_test= manual_test.set_index(\"Datum\")\n",
    "manual_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06807b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c63f6d",
   "metadata": {},
   "source": [
    "### change tt to manual_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bdf610a6",
   "metadata": {},
   "source": [
    "# Testing of the model (Linear Regression or Random Forest) through cross-validation.\n",
    "# The cross validation technique used is ShuffleSplit where model is trained over randomly selected 80% of the data and\n",
    "# tested over the remaining 20%. Model is run 5 times, R2 and RMSE is reported as the average of the five runs.\n",
    "\n",
    "tt= manual_train\n",
    "\n",
    "arr_r2 = []\n",
    "arr_RMSE = []\n",
    "\n",
    "list_feat = tt.columns.drop(predict)\n",
    "X = np.array(tt[list_feat])\n",
    "y = np.array(tt[[predict]])\n",
    "\n",
    "split_algo = ShuffleSplit(n_splits=5, test_size=0.20, random_state=0)\n",
    "\n",
    "for train_index, test_index in split_algo.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    algo.fit(X_train, y_train.ravel())\n",
    "    predictions = algo.predict(X_test)\n",
    "    \n",
    "    temp = r2_score(y_test, predictions)\n",
    "    arr_r2.append(temp)\n",
    "\n",
    "\n",
    "    temp2 = mean_squared_error(y_test, predictions, squared=False)\n",
    "    arr_RMSE.append(temp2)\n",
    "    \n",
    "r2 = np.average(arr_r2)\n",
    "rmse = np.average(arr_RMSE)\n",
    "\n",
    "print(\"Cross Validation results | \", test_model, \" | \" , station)\n",
    "print(\"Rsquared\", predict, \"\\n\", r2)\n",
    "print('RMSE ', predict, '\\n', rmse)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4078f31",
   "metadata": {},
   "source": [
    "%%notify\n",
    "%%time\n",
    "\n",
    "sklearn_r2 = make_scorer(r2_score, greater_is_better=True )\n",
    "X = np.array(manual_train[list_feat])\n",
    "y = np.array(manual_train[[predict]])\n",
    "\n",
    "split_algo = ShuffleSplit(test_size=0.20, random_state=0)\n",
    "cv_gen = split_algo.split(X, y)\n",
    "cv = list(cv_gen)\n",
    "\n",
    "sfs = SFS(algo,\n",
    "          k_features=(len(list_feat)),\n",
    "          forward=True,\n",
    "          floating=False, \n",
    "          scoring= sklearn_r2, \n",
    "          cv=cv)\n",
    "sfs = sfs.fit(X, y.ravel())\n",
    "\n",
    "fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n",
    "plt.title(\"SK R squared %s %s\" % (predict, station))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f03d5ac9",
   "metadata": {},
   "source": [
    "# Displaying the order in which features are entered in the model\n",
    "temp1_df = pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "display(temp1_df)\n",
    "temp3 = column_dict(manual_train[list_feat])\n",
    "feature_list =  order_of_features(temp1_df, temp3)\n",
    "print(feature_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9a3640f",
   "metadata": {},
   "source": [
    "feature_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model == \"lr\" and station == \"kahl\" and predict == \"NO3N\":\n",
    "    list_feat = [\"month_sin\",\"Temp\",\"pH\",\"week_sin\",\"Conduct\"]\n",
    "    \n",
    "if test_model == \"lr\" and station == \"kahl\" and predict == \"OPO4P\":\n",
    "    list_feat = [\"O2\",\"pH\",\"month_sin\", \"month_cos\", \"week_sin\"]\n",
    "\n",
    "if test_model == \"lr\" and station == \"kahl\" and predict == \"NH4N\":\n",
    "    list_feat = [\"week_cos\",\"Conduct\",\"pH\", \"week_sin\",\"month_sin\"]\n",
    "    \n",
    "if test_model == \"lr\" and station == \"erlabrunn\" and predict == \"NO3N\":\n",
    "    list_feat = [\"month_sin\",\"Temp\", \"Conduct\",\"week_sin\",\"month_cos\"]\n",
    "\n",
    "if test_model == \"lr\" and station == \"erlabrunn\" and predict == \"OPO4P\":\n",
    "    list_feat = [\"month_sin\",\"flow\",\"O2\",\"Temp\",\"Conduct\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d165fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model == \"rf\" and station == \"kahl\" and predict == \"NO3N\":\n",
    "    list_feat = [\"week_sin\",\"Conduct\",\"Temp\",\"O2\",\"month_sin\"]  \n",
    "\n",
    "if test_model == \"rf\" and station == \"kahl\" and predict == \"OPO4P\":\n",
    "    list_feat = [\"week_sin\",\"Conduct\",\"Temp\",\"flow\",\"O2\"]\n",
    "    \n",
    "if test_model == \"rf\" and station == \"kahl\" and predict == \"NH4N\":\n",
    "    list_feat = [\"week_sin\",\"Conduct\",\"Temp\",\"O2\",\"flow\"]\n",
    "    \n",
    "if test_model == \"rf\" and station == \"erlabrunn\" and predict == \"NO3N\":\n",
    "    list_feat = [\"week_sin\",\"Conduct\",\"Temp\",\"flow\",\"pH\"]\n",
    "\n",
    "if test_model == \"rf\" and station == \"erlabrunn\" and predict == \"OPO4P\":\n",
    "    list_feat = [\"week_sin\",\"flow\",\"Conduct\",\"Temp\",\"O2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd2202",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea379787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [50]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [10, 20, 30]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [6, 12, 20]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [6, 12, 20]\n",
    "\n",
    "\n",
    "# Create the param grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'bootstrap': bootstrap}\n",
    "print(param_grid)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rf_Grid = GridSearchCV(estimator = algo, param_grid = param_grid, cv = 5, verbose=2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119db939",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(manual_train[list_feat])\n",
    "y = np.array(manual_train[[predict]])\n",
    "\n",
    "rf_Grid.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059efbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper = rf_Grid.best_params_\n",
    "print(best_hyper)\n",
    "\n",
    "fitted_rf = RandomForestRegressor(bootstrap = best_hyper.get('bootstrap'), \n",
    "                                  max_depth= best_hyper.get('max_depth'),\n",
    "                                  max_features= best_hyper.get(\"max_features\"),\n",
    "                                  min_samples_leaf= best_hyper.get(\"min_samples_leaf\"),\n",
    "                                  min_samples_split = best_hyper.get(\"min_samples_split\") ,\n",
    "                                  n_estimators=100, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ef665",
   "metadata": {},
   "source": [
    "### Testing on the unseen dataset i.e the 20% of the earlier separated dataset. The model is trained on the top 5 features that are mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba63178",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = manual_train\n",
    "tt = manual_test\n",
    "\n",
    "arr_r2 = []\n",
    "arr_RMSE = []\n",
    "\n",
    "# list_feat = manual_train.columns.drop(predict)\n",
    "\n",
    "    #train\n",
    "X_train = np.array(manual_train[list_feat])\n",
    "y_train = np.array(manual_train[[predict]])\n",
    "\n",
    "    #test\n",
    "X_test = np.array(manual_test[list_feat])\n",
    "y_test = np.array(manual_test[[predict]])\n",
    "\n",
    "model = fitted_rf\n",
    "model.fit(X_train,  y_train.ravel())\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "temp_r2 = r2_score(y_test, predictions)\n",
    "arr_r2.append(temp_r2)\n",
    "\n",
    "temp_rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "arr_RMSE.append(temp_rmse)\n",
    "\n",
    "print(\"Rsquared\", predict, \"\\n\", temp_r2)\n",
    "print('RMSE ', predict, '\\n', temp_rmse)\n",
    "print( \"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the oberved vs predicted curve for the run on testing dataset one cell above\n",
    "\n",
    "rr = pd.DataFrame(predictions, columns = {\"Predicted_%s\" %(predict)}).round(2)\n",
    "rr =  pd.concat([manual_test[[predict]].reset_index(),rr], axis =1 ).sort_values('Datum').set_index(\"Datum\")\n",
    "\n",
    "\n",
    "figx = px.line(rr, x = rr.index, y=[predict, \"Predicted_%s\" %(predict)], title = \"%s\" %(station))\n",
    "figx.update_xaxes(rangeslider_visible = True)\n",
    "# figx.update_layout(yaxis_range=[0, 0.2])\n",
    "print(\"full\")\n",
    "print(\"Rsquared\", predict, \"\\n\", temp_r2)\n",
    "print('RMSE ', predict, '\\n', temp_rmse)\n",
    "figx.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953abaf",
   "metadata": {},
   "source": [
    "### Determination of Knee"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cd843c4",
   "metadata": {},
   "source": [
    "station = \"kahl\"\n",
    "predict = \"NH4N\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "09d0dd07",
   "metadata": {},
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a00f7d38",
   "metadata": {},
   "source": [
    "results = pd.read_csv(r'%s_%s_numbers.csv' %(station, predict)) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "94c53436",
   "metadata": {},
   "source": [
    "inp = 0.0009\n",
    "# sample_per_mon = inp * 30*24*3600\n",
    "# sample_per_mon = 2400\n",
    "minute_frequency = 30*24*60 /sample_per_mon\n",
    "print(\"minutes: \", minute_frequency)\n",
    "print(\"hours: \", minute_frequency/60)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f94568a4",
   "metadata": {},
   "source": [
    "minu = 5*60\n",
    "frequency = 1/(minu*60)\n",
    "print(frequency)\n",
    "print(\"number of divisions: \", frequency/0.0002)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3af5fd2",
   "metadata": {},
   "source": [
    "freq_m = freq*30*24*60*60\n",
    "print(\"Samples per month: \", freq_m)\n",
    "print(\"Hourly frequency: \", round(30/freq_m*24, 10), \" hours\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cb056ff",
   "metadata": {},
   "source": [
    "x = np.array(results[[\"Samples Per Month\"]].div(2592000)).squeeze()\n",
    "y = np.array(results[[\"R2\"]]).squeeze()\n",
    "# kneedle = kneed.KneeLocator(x, y, S=9.0, curve=\"convex\", direction=\"decreasing\")\n",
    "kneedle = kneed.KneeLocator(x, y, S=9.0, curve=\"concave\", direction=\"increasing\")\n",
    "freq = round(kneedle.knee, 10)\n",
    "print('%s | %s' %(station, predict))\n",
    "print (\"Frequency: \", freq)\n",
    "freq_m = freq*30*24*60*60\n",
    "# print(\"Samples per month: \", freq_m)\n",
    "print(\"Hourly frequency: \", round(30/freq_m*24, 10), \" hours\")\n",
    "kneedle.plot_knee()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8f325fd",
   "metadata": {},
   "source": [
    "### Time Series of the optimal series"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e1d755a",
   "metadata": {},
   "source": [
    "if predict == \"NH4N\" or predict == \"OPO4P\":\n",
    "    freq = 0.0001157407\n",
    "elif predict == \"NO3N\":\n",
    "    freq = 5.78704 / 100000\n",
    "    \n",
    "samples = int(freq * 2592000)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_r2 = []\n",
    "arr_RMSE = []\n",
    "samples_per_month = [1, 3, 5, 10, 20, 30, 80, 100,120, 150, 175, 180, 190, 200, 210, 225, 250, 275, 300, 325, 350,\n",
    "                     375, 400, 425, 450, 475, 500,600,700, 800, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700,\n",
    "                     1800, 2001, 2200, 2400]\n",
    "# samples_per_month = [samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad589927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the vanilla RF model on 40 different sampling frequencies and testing them on the 20% of the entire dataset.\n",
    "for j in samples_per_month:\n",
    "    ss = manual_train.reset_index()\n",
    "    tt = manual_test\n",
    "    \n",
    "    boss_list = [x for x in ss.columns]\n",
    "    time_df = pd.DataFrame(columns= boss_list)\n",
    "\n",
    "    last_date = ss.iloc[-2,0]\n",
    "    temp_date = ss.iloc[0,0]\n",
    "    i = 0\n",
    "    while temp_date.month != last_date.month or temp_date.year != last_date.year:\n",
    "        temp_date = ss.iloc[0,0] + pd.DateOffset(months =i)\n",
    "        temp = ss[(ss.Datum.dt.month == temp_date.month) & (ss.Datum.dt.year==temp_date.year) ]\n",
    "#         print(\"Number of data points in \", temp_date.month, \" : \", len(temp) )\n",
    "        \n",
    "        if len(temp) > j:\n",
    "            temp2 = temp.sample(n = j, random_state = 1)\n",
    "        elif len(temp) < j:\n",
    "            temp2 = temp.sample(frac = 0.95, random_state = 1)\n",
    "            \n",
    "        time_df = pd.concat([time_df, temp2], axis = 0)\n",
    "        i = i+1\n",
    "\n",
    "    time_df = time_df.drop_duplicates(subset=['Datum'], keep='first')\n",
    "#     dates = np.array(time_df[\"Datum\"])\n",
    "    time_df = time_df.set_index(\"Datum\")\n",
    "#     ss = ss.set_index(\"Datum\")\n",
    "#     ss.drop(dates, inplace=True)\n",
    "    \n",
    "#     display(ss)\n",
    "#     display(time_df)\n",
    "\n",
    "#     list_feat = time_df.columns.drop(predict)   NOT USING THIS JUST TO SEE HOW MUCH IT CHANGES IF WE USE LESSER PREDICTORS\n",
    "\n",
    "    #train\n",
    "    X_train = np.array(time_df[list_feat])\n",
    "    y_train = np.array(time_df[[predict]])\n",
    "\n",
    "    #test\n",
    "    X_test = np.array(tt[list_feat])\n",
    "    y_test = np.array(tt[[predict]])\n",
    "\n",
    "    model = RandomForestRegressor(n_jobs = -1)\n",
    "    model.fit(X_train,  y_train.ravel())\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    temp_r2 = r2_score(y_test, predictions)\n",
    "    arr_r2.append(temp_r2)\n",
    "\n",
    "    temp_rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "    arr_RMSE.append(temp_rmse)\n",
    "\n",
    "    print(j)\n",
    "    print(\"Rsquared\", predict, \"\\n\", temp_r2)\n",
    "    print('RMSE ', predict, '\\n', temp_rmse)\n",
    "    print( \"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4599fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the result of 40 runs in a pandas dataframe\n",
    "results = pd.DataFrame(list(zip(samples_per_month, arr_r2, arr_RMSE)),\n",
    "               columns =['Samples Per Month', 'R2', 'RMSE'])\n",
    "results = results.sort_values('Samples Per Month')\n",
    "results.to_csv(r'%s_%s_numbers_3_more_freq.csv' %(station, predict), index = False, header = True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4559791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determination of knee using kneedle algorithm\n",
    "\n",
    "x = np.array(results[[\"Samples Per Month\"]].div(2592000)).squeeze()\n",
    "y = np.array(results[[\"RMSE\"]]).squeeze()\n",
    "\n",
    "kneedle = kneed.KneeLocator(x, y, S=9.0, curve=\"convex\", direction=\"decreasing\")\n",
    "freq = round(kneedle.knee, 10)  # knee point frequency in Hz\n",
    "\n",
    "print(station, predict)\n",
    "print (\"Frequency: \", freq)\n",
    "\n",
    "freq_m = freq*30*24*60*60   # Converting the frequency in Hz to samples per month\n",
    "print(\"Samples per month: \", freq_m)   \n",
    "\n",
    "print(\"Hourly frequency: \", round(30/freq_m*24, 10), \" hours\") # Converting the frequency in Hz to hourly\n",
    "\n",
    "kneedle.plot_knee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210845b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = tt\n",
    "\n",
    "rr = pd.DataFrame(predictions, columns = {\"Predicted_%s\" %(predict)}).round(2)\n",
    "rr =  pd.concat([tt[[predict]].reset_index(),rr], axis =1 ).sort_values('Datum').set_index(\"Datum\")\n",
    "\n",
    "figx = px.line(rr, x = rr.index, y=[predict, \"Predicted_%s\" %(predict)], title = \"%s \" %(station))\n",
    "figx.update_xaxes(rangeslider_visible = True)\n",
    "# figx.update_layout(yaxis_range=[0, 0.2])\n",
    "\n",
    "print(samples_per_month)      #no use, it's here just for marking\n",
    "print(\"Rsquared\", predict, \"\\n\", temp_r2)\n",
    "print('RMSE ', predict, '\\n', temp_rmse)\n",
    "\n",
    "figx.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3eff6c0f",
   "metadata": {},
   "source": [
    "tt = tt\n",
    "\n",
    "rr = pd.DataFrame(predictions, columns = {\"Predicted_%s\" %(predict)}).round(2)\n",
    "rr =  pd.concat([tt[[predict]].reset_index(),rr], axis =1 ).sort_values('Datum').set_index(\"Datum\")\n",
    "\n",
    "figx = px.line(rr, x = rr.index, y=[predict, \"Predicted_%s\" %(predict)], title = \"%s \" %(station))\n",
    "figx.update_xaxes(rangeslider_visible = True)\n",
    "# figx.update_layout(yaxis_range=[0, 0.2])\n",
    "print(samples_per_month)      #no use, it's here just for marking\n",
    "print(\"Rsquared\", predict, \"\\n\", temp_r2)\n",
    "print('RMSE ', predict, '\\n', temp_rmse)\n",
    "figx.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
